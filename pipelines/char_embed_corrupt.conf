#!../pimlico.sh
# Language model-type neural network that looks at varying amounts of symbol context to learn symbol representations.
#
# The model is described in:
#   Unsupervised Learning of Cross-Lingual Symbol Embeddings Without Parallel Data
#   Mark Granroth-Wilding and Hannu Toivonen (2019)
#   In proceedings Society for Computation in Linguistics (SCiL)
#
# In the paper, the model is called Xsym. In this code, it is called neural_sixgram.
#
# This pipeline implements the language corruption experiments reported in the paper.
# It takes real language data (Finnish forum posts) and applies random corruptions to
# it, training Xsym on uncorrupted and corrupted pairs.
#
# See pipeline char_embed_corpora.conf for details of datasets.

[pipeline]
name=char_embed_corrupt
release=0.9
python_path=%(project_root)s/src/python

[vars]
ylilauta_path=%(home)s/data/ylilauta_20150304.vrt


################# Dataset preparation ################
### Ylilauta ###

# A Finnish forum corpus. Informal, messy, colloquial written Finnish
[ylilauta]
type=langsim.modules.input.ylilauta
files=%(ylilauta_path)s
modvar_lang="fi"

# Group into fixed-size archives
[ylilauta_grouped]
type=pimlico.modules.corpora.tar_filter

# Limit the size of the corpus, since in the future we'll be forced to work with smaller
# datasets for low-resourced languages, so we want to know the methods work
[ylilauta_cut]
type=pimlico.modules.corpora.subset
input=ylilauta_grouped
size=190K
skip_invalid=T

# Workaround for inheritance problem: map tokenized data to plain text for char tokenizing
# (Solved in a later version of Pimlico)
[ylilauta_text]
type=pimlico.modules.text.untokenize
input=ylilauta_cut
filter=T

# Transform to char-level split text
[ylilauta_chars]
type=pimlico.modules.text.char_tokenize
filter=T


# Build a vocab of the characters we need
[char_vocab]
type=pimlico.modules.corpora.vocab_builder
input=ylilauta_chars
# Throw away chars we see fewer than 500 times in the corpus: they're probably just junk
threshold=500
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=ylilauta_chars
input_vocab=char_vocab
oov=OOV

# Compute frequency distribution for vocab
[freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=ids
input_vocab=char_vocab


################# Corpus corruption ################
# Corrupt the corpus in various ways and to various levels to produce
# "new language" corpora

# Take a subset of the corpus to keep uncorrupted
# then apply corruption to a separate subset
[ids_split]
type=pimlico.modules.corpora.split
input=ids
set1_size=0.5

# Part of the original corpus that won't be corrupted, so that our
# original language and invented (corrupted) language use different
# samples of the corpus
[uncorrupted_ids]
type=pimlico.modules.utility.alias
input=ids_split.set1

[uncorrupted_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=uncorrupted_ids
input_vocab=char_vocab

# Part of the original corpus that will be fed into the corruption
# methods to produce artificial language data
[ids_for_corruption]
type=pimlico.modules.utility.alias
input=ids_split.set2


########### Different levels of corruption
[corrupt]
type=langsim.modules.fake_language.corrupt
input_corpus=ids_for_corruption
input_vocab=char_vocab
input_frequencies=freqs
alt_naming=pos
char_subst_prop=0|0.15|0.3
char_split_prop=0|0.15|0.3
char_map_prop=0|0.15|0.3

[corrupt_freqs]
type=pimlico.modules.corpora.vocab_counter
tie_alts=T
input_corpus=corrupt.corpus
input_vocab=corrupt.vocab

[sixgram_corrupt]
type=langsim.modules.local_lm.neural_sixgram
tie_alts=T
input_vocabs=char_vocab,corrupt.vocab
input_corpora=uncorrupted_ids,corrupt
input_frequencies=uncorrupted_freqs,corrupt_freqs
input_mapped_pairs=corrupt.close_pairs
oov=OOV
batch=1000
embedding_size=30
composition2_layers=
composition3_layers=
embedding_activation=linear
predictor_layers=30
context_weights=0,1,1,1,1,1
epochs=10
unit_norm=T
dropout=0.1
composition_dropout=0.01
modvar_lang_pair=join("_", corpora.lang)
# Plot occasionally, but not too often
plot_freq=2000
# Compute the overlap similarity after each 500 batches
sim_freq=500

[sixgram_metrics_corr]
type=langsim.modules.local_lm.val_crit_correlation
input_models=*sixgram_corrupt


####### Now with validation criterion used for early stopping

[sixgram_corrupt_vc_data]
type=langsim.modules.local_lm.neural_sixgram_samples
tie_alts=T
input_vocabs=char_vocab,corrupt.vocab
input_corpora=uncorrupted_ids,corrupt
input_frequencies=uncorrupted_freqs,corrupt_freqs
oov=OOV
modvar_corruption=altname(input_corpora)
modvar_lang_pair=join("_", corpora.lang)

[sixgram_corrupt_vc]
type=langsim.modules.local_lm.neural_sixgram2
tie_alts=T
input_vocabs=char_vocab,corrupt.vocab
input_samples=sixgram_corrupt_vc_data
input_mapped_pairs=corrupt.close_pairs
batch=1000
embedding_size=30
composition2_layers=
composition3_layers=
predictor_layers=30
epochs=10
unit_norm=T
dropout=0.1
composition_dropout=0.01
# Plot after every epoch
plot_freq=0
# Restart 5 times and use the result that gets the best validation criterion value
restarts=5

[corrupt_vc_results]
type=langsim.modules.local_lm.corruption_results
tie_alts=T
input_models=*sixgram_corrupt_vc.model
input_vocab1s=char_vocab*$(len(corruption))
input_vocab2s=*corrupt.vocab
input_corruption_params=*corrupt.corruption_params
input_mapped_pairs=*corrupt.close_pairs
