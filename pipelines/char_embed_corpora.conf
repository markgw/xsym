#!../pimlico.sh
# Language model-type neural network that looks at varying amounts of symbol context to learn symbol representations.
#
# The model is described in:
#   Unsupervised Learning of Cross-Lingual Symbol Embeddings Without Parallel Data
#   Mark Granroth-Wilding and Hannu Toivonen (2019)
#   In proceedings Society for Computation in Linguistics (SCiL)
#
# In the paper, the model is called Xsym. In this code, it is called neural_sixgram.
#
# There are two slightly different implementations of the training code, found in the
# Pimlico modules neural_sixgram and neural_sixgram2. If you're training the model
# yourself, you should use the more recent and more efficient neural_sixgram2.
#
# This pipeline loads a variety of corpora and trains Xsym on them. It produces all the
# models described in the paper above. To train on these corpora, you'll need to download
# them and then update the paths in the [vars] section below to point to their locations.
#
# The pipeline also includes training on some language pairs not reported in the
# SCiL 2019 paper.
#
# Corpora
# =======
# Ylilauta:
#   Finnish forum posts.
#   Download http://urn.fi/urn:nbn:fi:lb-2016101211
#
# Estonian Reference Corpus:
#   Corpus of written Estonian from a variety of sources.
#   Here we use just the subsets: tasakaalus_ajalehed and foorumid_lausestatud.
#   Download from http://www.cl.ut.ee/korpused/segakorpus/.
#   Download the forum post subset from http://www.cl.ut.ee/korpused/segakorpus/uusmeedia/foorumid.
#
# Danish Wikipedia dump:
#   Text dump of Danish Wikipedia.
#   Download from http://linguatools.org/tools/corpora/wikipedia-monolingual-corpora/.
#
# Europarl:
#   The Europarl corpus of transcripts from the European Parliament.
#   Homepage: http://www.statmt.org/europarl/
#   Download the full source release. Below, we use the Swedish, Spanish and Portuguese parts.
#
# Multilingual Resource Collection of the University of Helsinki Language Corpus Server (UHLCS):
#   Homepage: http://www.ling.helsinki.fi/uhlcs/
#   Data used to be available there, but is now available through the CSC: https://www.csc.fi/
#   You'll need to request access to the specific language datasets used below.
#   The data you get is messy, in inconsistent formats and encodings. See the code distributed
#   separately at
#      https://mark.granroth-wilding.co.uk/papers/unsup_symbol/
#   for how to preprocess this and get it into a useable textual form, which we use below.

[pipeline]
name=char_embed_corpora
release=0.9
python_path=%(project_root)s/src/python

[vars]
# Paths to different corpora
# Ylilauta extracted VRT file
ylilauta_path=%(home)s/data/ylilauta_20150304.vrt
# Estonian Reference Corpus, tasakaalus_ajalehed subset directory
est_ref_path=%(home)s/data/estonian_ref/tasakaalus_ajalehed
# Estonian Reference Corpus, foorumid_lausestatud subset directory
est_forum_path=%(home)s/data/estonian_ref/foorumid_lausestatud
# Danish Wikipedia dump, XML file
dan_wiki_path=%(home)s/data/dan_wiki/dawiki-20140725-corpus.xml
# Europarl: we only use Swedish, but this is the path to the directory containing all of the languages
europarl_dir=/proj/magranro/data/europarl/corpus
# Paths to UHLCS data, preprocessed by a separate pipeline
uhlcs_path=%(home)s/data/uhlcs_processed


################# Dataset preparation ################
### Ylilauta ###

# A Finnish forum corpus. Informal, messy, colloquial written Finnish
[ylilauta]
type=langsim.modules.input.ylilauta
files=%(ylilauta_path)s
modvar_lang="fi"

# Group into fixed-size archives
[ylilauta_grouped]
type=pimlico.modules.corpora.tar_filter

# Compute some corpus stats so we can compare the sizes of the corpora
[ylilauta_stats]
type=pimlico.modules.corpora.corpus_stats

# Limit the size of the corpus, choosing a size on the basis of the computed stats, so Fi and Et are similar
# After this, this corpus has around the same number of tokens as Est Ref, below
[ylilauta_cut]
type=pimlico.modules.corpora.subset
input=ylilauta_grouped
size=190K
skip_invalid=T

# Recompute the stats
[ylilauta_cut_stats]
type=pimlico.modules.corpora.corpus_stats

# Lower-case everything
[ylilauta_norm]
type=pimlico.modules.text.normalize
input=ylilauta_cut
case=lower
filter=T

# Workaround for inheritance problem: map tokenized data to plain text for char tokenizing
[ylilauta_text]
type=pimlico.modules.text.untokenize
input=ylilauta_norm
filter=T

# Transform to char-level split text
[ylilauta_chars]
type=pimlico.modules.text.char_tokenize
filter=T

# Build a vocab of the characters we need
[ylilauta_char_vocab]
type=pimlico.modules.corpora.vocab_builder
# Throw away chars we see fewer than 500 times in the corpus: they're probably just junk
threshold=500
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[ylilauta_ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=ylilauta_chars
input_vocab=ylilauta_char_vocab
oov=OOV

# Compute unigram distribution
[ylilauta_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=ylilauta_ids
input_vocab=ylilauta_char_vocab


### Estonian Reference Corpus ###

# One portion from the balanced version of the Estonian Reference Corpus
# http://www.cl.ut.ee/korpused/segakorpus/
[est_ref]
type=pimlico.modules.input.xml
files=%(est_ref_path)s/*.tei
document_node_type=text
modvar_lang="et"

[est_ref_grouped]
type=pimlico.modules.corpora.tar_filter

# Perform simple text normalization
# The data's still quite messy after this, and we end up splitting some lines where we shouldn't,
# but it's probably good enough for our purposes
[est_ref_norm]
type=langsim.modules.input.est_ref_normalize
filter=T

# The text is already tokenized: just split on spaces
[est_ref_tokens]
type=pimlico.modules.text.simple_tokenize

# Compute some corpus stats so we can compare the sizes of the corpora
# This is the smaller corpus, so we don't need to cut it
[est_ref_stats]
type=pimlico.modules.corpora.corpus_stats

# Untokenize the tokenized text. This deals with whitespace strangeness
[est_ref_text]
type=pimlico.modules.text.untokenize
input=est_ref_tokens
filter=T

# Transform to char-level split text
[est_ref_chars]
type=pimlico.modules.text.char_tokenize
input=est_ref_text
filter=T

# Build a vocab of the characters we need
[est_ref_char_vocab]
type=pimlico.modules.corpora.vocab_builder
# Throw away chars we see fewer than 500 times in the corpus: they're probably just junk
threshold=500
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[est_ref_ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=est_ref_chars
input_vocab=est_ref_char_vocab
oov=OOV

# Compute unigram distribution
[est_ref_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=est_ref_ids
input_vocab=est_ref_char_vocab




### Estonian Reference Corpus: forum posts ###

# One portion from the Estonian Reference Corpus, specifically forum posts
# http://www.cl.ut.ee/korpused/segakorpus/uusmeedia/foorumid
[est_forum]
type=pimlico.modules.input.xml
files=%(est_forum_path)s/*.tei
document_node_type=text
modvar_lang="et"

[est_forum_grouped]
type=pimlico.modules.corpora.tar_filter

[est_forum_norm]
type=langsim.modules.input.est_ref_normalize
filter=T
forum=T

[est_forum_tokens]
type=pimlico.modules.text.simple_tokenize

[est_forum_stats]
type=pimlico.modules.corpora.corpus_stats

# Untokenize the tokenized text. This deals with whitespace strangeness
[est_forum_text]
type=pimlico.modules.text.untokenize
input=est_forum_tokens
filter=T

# Transform to char-level split text
[est_forum_chars]
type=pimlico.modules.text.char_tokenize
input=est_forum_text
filter=T

# Build a vocab of the characters we need
[est_forum_char_vocab]
type=pimlico.modules.corpora.vocab_builder
# Throw away chars we see fewer than 500 times in the corpus: they're probably just junk
threshold=500
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[est_forum_ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=est_forum_chars
input_vocab=est_forum_char_vocab
oov=OOV

# Compute unigram distribution
[est_forum_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=est_forum_ids
input_vocab=est_forum_char_vocab




### Danish Wikipedia Corpus ###

# Danish Wikipedia dump, downloaded from: http://linguatools.org/tools/corpora/wikipedia-monolingual-corpora/
[dan_wiki]
type=pimlico.modules.input.xml
files=%(dan_wiki_path)s
document_node_type=article
document_name_attr=name
modvar_lang="da"

[dan_wiki_grouped]
type=pimlico.modules.corpora.tar_filter

[dan_wiki_norm]
type=pimlico.modules.text.text_normalize
strip=T
case=lower
blank_lines=T

# Tokenize just for computing stats
[dan_wiki_tokens]
type=pimlico.modules.text.simple_tokenize
filter=T

[dan_wiki_stats]
type=pimlico.modules.corpora.corpus_stats

# Transform to char-level split text
[dan_wiki_chars]
type=pimlico.modules.text.char_tokenize
input=dan_wiki_norm
filter=T

# Build a vocab of the characters we need
[dan_wiki_char_vocab]
type=pimlico.modules.corpora.vocab_builder
# Throw away chars we see fewer than 500 times in the corpus: they're probably just junk
threshold=500
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[dan_wiki_ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=dan_wiki_chars
input_vocab=dan_wiki_char_vocab
oov=OOV

# Compute unigram distribution
[dan_wiki_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=dan_wiki_ids
input_vocab=dan_wiki_char_vocab



### Europarl Swedish ###
[sv_europarl]
type=langsim.modules.input.europarl
files=%(europarl_dir)s/sv/*
modvar_lang="sv"
encoding=utf-8
encoding_errors=ignore

[sv_europarl_grouped]
type=pimlico.modules.corpora.tar_filter

[sv_europarl_norm]
type=pimlico.modules.text.text_normalize
strip=T
case=lower
blank_lines=T

# Tokenize just for computing stats
[sv_europarl_tokens]
type=pimlico.modules.text.simple_tokenize
filter=T

[sv_europarl_stats]
type=pimlico.modules.corpora.corpus_stats

# Limit the size of the corpus, choosing a size on the basis of the computed stats, so da and sv are similar
# After this, this corpus has around the same number of tokens as dan_wiki, above
# This has 884,988 tokens, dan_wiki has 888,404
[sv_europarl_cut]
type=pimlico.modules.corpora.subset
input=sv_europarl_norm
size=19
skip_invalid=T

# Tokenize just for computing stats
[sv_europarl_cut_tokens]
type=pimlico.modules.text.simple_tokenize
filter=T

[sv_europarl_cut_stats]
type=pimlico.modules.corpora.corpus_stats

# Transform to char-level split text
[sv_europarl_chars]
type=pimlico.modules.text.char_tokenize
input=sv_europarl_cut
filter=T

# Build a vocab of the characters we need
[sv_europarl_char_vocab]
type=pimlico.modules.corpora.vocab_builder
# Throw away chars we see fewer than 500 times in the corpus: they're probably just junk
threshold=500
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[sv_europarl_ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=sv_europarl_chars
input_vocab=sv_europarl_char_vocab
oov=OOV

# Compute unigram distribution
[sv_europarl_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=sv_europarl_ids
input_vocab=sv_europarl_char_vocab




### UHLCS corpus for Dvina ###
# Pre-processed UHLCS dataset to deal with encoding problems, etc
# ~200k tokens total
[dvina]
type=pimlico.datatypes.tar.RawTextTarredCorpus
dir=%(uhlcs_path)s/dvina
modvar_lang="dvi"

[dvina_norm]
type=pimlico.modules.text.text_normalize
strip=T
case=lower
blank_lines=T

# Transform to char-level split text
[dvina_chars]
type=pimlico.modules.text.char_tokenize
filter=T

# Build a vocab of the characters we need
[dvina_char_vocab]
type=pimlico.modules.corpora.vocab_builder
# Use a lower threshold than on the earlier corpora, since this is so small
threshold=200
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[dvina_ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=dvina_chars
input_vocab=dvina_char_vocab
oov=OOV

# Compute unigram distribution
[dvina_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=dvina_ids
input_vocab=dvina_char_vocab



### UHLCS corpus for Livvi ###
# Pre-processed UHLCS dataset to deal with encoding problems, etc
# ~153k tokens total
[livvi]
type=pimlico.datatypes.tar.RawTextTarredCorpus
dir=%(uhlcs_path)s/livvi
modvar_lang="liv"

[livvi_norm]
type=pimlico.modules.text.text_normalize
strip=T
case=lower
blank_lines=T

# Transform to char-level split text
[livvi_chars]
type=pimlico.modules.text.char_tokenize
filter=T

# Build a vocab of the characters we need
[livvi_char_vocab]
type=pimlico.modules.corpora.vocab_builder
# Use a lower threshold than on the earlier corpora, since this is so small
threshold=200
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[livvi_ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=livvi_chars
input_vocab=livvi_char_vocab
oov=OOV

# Compute unigram distribution
[livvi_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=livvi_ids
input_vocab=livvi_char_vocab



### UHLCS corpus for Ingrian ###
# Pre-processed UHLCS dataset to deal with encoding problems, etc
# ~36k tokens total
# This is a super-endangered language, with only ~130 speakers
[ingrian]
type=pimlico.datatypes.tar.RawTextTarredCorpus
dir=%(uhlcs_path)s/ingrian
modvar_lang="ing"

[ingrian_norm]
type=pimlico.modules.text.text_normalize
strip=T
case=lower
blank_lines=T

# Transform to char-level split text
[ingrian_chars]
type=pimlico.modules.text.char_tokenize
filter=T

# Build a vocab of the characters we need
[ingrian_char_vocab]
type=pimlico.modules.corpora.vocab_builder
# Use a lower threshold than on the earlier corpora, since this is so small
threshold=200
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[ingrian_ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=ingrian_chars
input_vocab=ingrian_char_vocab
oov=OOV

# Compute unigram distribution
[ingrian_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=ingrian_ids
input_vocab=ingrian_char_vocab




################# Sixgram model training ################

# Train representations by training an unusual sort of language model, learning representations of unigrams,
# bigrams and trigrams in the same vector space
[sixgram_data]
type=langsim.modules.local_lm.neural_sixgram_samples
input_vocabs=ylilauta_char_vocab,est_ref_char_vocab
input_corpora=ylilauta_ids,est_ref_ids
oov=OOV
input_frequencies=ylilauta_freqs,est_ref_freqs

[sixgram]
type=langsim.modules.local_lm.neural_sixgram2
input_vocabs=ylilauta_char_vocab,est_ref_char_vocab
input_samples=sixgram_data
batch=1000
embedding_size=30
composition2_layers=
composition3_layers=
predictor_layers=30
epochs=10
unit_norm=T
dropout=0.1
composition_dropout=0.01
modvar_lang_pair=join("_", lang)
# Plot after every epoch
plot_freq=0
# Restart 10 times and use the result that gets the best validation criterion value
restarts=10
# Increase patience of early stopping
patience=4
# Since it's quite a large dataset, split up epochs so we evaluate early stopping criterion more often
split_epochs=5

[plot_sixgram]
type=langsim.modules.local_lm.plot
input_model=sixgram
input_vocabs=ylilauta_char_vocab,est_ref_char_vocab
input_corpora=ylilauta_ids,est_ref_ids
input_frequencies=ylilauta_freqs,est_ref_freqs
distance=cos
min_token_prop=0.005
lang_names=fi,et

[sixgram_anal]
type=langsim.modules.local_lm.embed_anal
input_model=sixgram
input_vocabs=ylilauta_char_vocab,est_ref_char_vocab
input_frequencies=ylilauta_freqs,est_ref_freqs
min_token_prop=0.005
lang_names=fi,et
oov=OOV

[sixgram_embeddings]
type=langsim.modules.local_lm.embeddings_from_model
input_model=sixgram
input_vocabs=ylilauta_char_vocab,est_ref_char_vocab
input_frequencies=ylilauta_freqs,est_ref_freqs
lang_names=fi,et

[w2v_embeddings_fi_et_mixed]
type=pimlico.modules.embeddings.store_word2vec

[tsv_embeddings_fi_et_mixed]
type=langsim.modules.local_lm.store_tsv
input=sixgram_embeddings



# Train on Fin and Est forum data
[sixgram_forum_data]
type=langsim.modules.local_lm.neural_sixgram_samples
input_vocabs=ylilauta_char_vocab,est_forum_char_vocab
input_corpora=ylilauta_ids,est_forum_ids
input_frequencies=ylilauta_freqs,est_forum_freqs
oov=OOV

[sixgram_forum]
type=langsim.modules.local_lm.neural_sixgram2
input_vocabs=ylilauta_char_vocab,est_forum_char_vocab
input_samples=sixgram_forum_data
batch=1000
embedding_size=30
composition2_layers=
composition3_layers=
predictor_layers=30
epochs=10
unit_norm=T
dropout=0.1
composition_dropout=0.01
modvar_lang_pair=join("_", lang)
# Plot after every epoch
plot_freq=0
# Restart 10 times and use the result that gets the best validation criterion value
restarts=10
# Increase patience of early stopping
patience=4
# Since it's quite a large dataset, split up epochs so we evaluate early stopping criterion more often
split_epochs=5


[plot_sixgram_forum]
type=langsim.modules.local_lm.plot
input_model=sixgram_forum
input_vocabs=ylilauta_char_vocab,est_forum_char_vocab
input_corpora=ylilauta_ids,est_forum_ids
input_frequencies=ylilauta_freqs,est_forum_freqs
distance=cos
min_token_prop=0.005
lang_names=fi,et

[sixgram_forum_anal_frequent]
type=langsim.modules.local_lm.embed_anal
input_model=sixgram_forum
input_vocabs=ylilauta_char_vocab,est_forum_char_vocab
input_frequencies=ylilauta_freqs,est_forum_freqs
min_token_prop=0.005
lang_names=fi,et
oov=OOV

[sixgram_forum_anal]
type=langsim.modules.local_lm.embed_anal
input_model=sixgram_forum
input_vocabs=ylilauta_char_vocab,est_forum_char_vocab
input_frequencies=ylilauta_freqs,est_forum_freqs
min_token_prop=0.
lang_names=fi,et
oov=OOV

[sixgram_forum_embeddings]
type=langsim.modules.local_lm.embeddings_from_model
input_model=sixgram_forum
input_vocabs=ylilauta_char_vocab,est_forum_char_vocab
input_frequencies=ylilauta_freqs,est_forum_freqs
lang_names=fi,et

[w2v_embeddings_fi_et_forum]
type=pimlico.modules.embeddings.store_word2vec

[tsv_embeddings_fi_et_forum]
type=langsim.modules.local_lm.store_tsv
input=sixgram_forum_embeddings




# Train on Danish Wikipedia corpus and Swedish Europarl
[sixgram_da_sv_data]
type=langsim.modules.local_lm.neural_sixgram_samples
input_vocabs=dan_wiki_char_vocab,sv_europarl_char_vocab
input_corpora=dan_wiki_ids,sv_europarl_ids
input_frequencies=dan_wiki_freqs,sv_europarl_freqs
oov=OOV

[sixgram_da_sv]
type=langsim.modules.local_lm.neural_sixgram2
input_vocabs=dan_wiki_char_vocab,sv_europarl_char_vocab
input_samples=sixgram_da_sv_data
batch=1000
embedding_size=30
composition2_layers=
composition3_layers=
predictor_layers=30
# Max epochs
epochs=30
unit_norm=T
dropout=0.1
composition_dropout=0.01
modvar_lang_pair=join("_", lang)
# Restart 10 times and use the result that gets the best validation criterion value
restarts=10
# Increase patience of early stopping
patience=4


[plot_sixgram_da_sv]
type=langsim.modules.local_lm.plot
input_model=sixgram_da_sv
input_vocabs=dan_wiki_char_vocab,sv_europarl_char_vocab
input_corpora=dan_wiki_ids,sv_europarl_ids
input_frequencies=dan_wiki_freqs,sv_europarl_freqs
distance=cos
min_token_prop=0.005
lang_names=da,sv

[sixgram_da_sv_anal]
type=langsim.modules.local_lm.embed_anal
input_model=sixgram_da_sv
input_vocabs=dan_wiki_char_vocab,sv_europarl_char_vocab
input_frequencies=dan_wiki_freqs,sv_europarl_freqs
min_token_prop=0.005
lang_names=da,sv
oov=OOV

[sixgram_da_sv_embeddings]
type=langsim.modules.local_lm.embeddings_from_model
input_model=sixgram_da_sv
input_vocabs=dan_wiki_char_vocab,sv_europarl_char_vocab
input_frequencies=dan_wiki_freqs,sv_europarl_freqs
lang_names=da,sv

[w2v_embeddings_da_sv]
type=pimlico.modules.embeddings.store_word2vec
input=sixgram_da_sv_embeddings

[tsv_embeddings_da_sv]
type=langsim.modules.local_lm.store_tsv
input=sixgram_da_sv_embeddings



# Train on Dvina and Livvi datasets from UHLCS
[sixgram_dvi_liv_data]
type=langsim.modules.local_lm.neural_sixgram_samples
input_vocabs=dvina_char_vocab,livvi_char_vocab
input_corpora=dvina_ids,livvi_ids
input_frequencies=dvina_freqs,livvi_freqs
oov=OOV

[sixgram_dvi_liv]
type=langsim.modules.local_lm.neural_sixgram2
input_vocabs=dvina_char_vocab,livvi_char_vocab
input_samples=sixgram_dvi_liv_data
# Max epochs
epochs=30
modvar_lang_pair=join("_", lang)
# Increase patience of early stopping
patience=4
restarts=10


[plot_sixgram_dvi_liv]
type=langsim.modules.local_lm.plot
input_model=sixgram_dvi_liv
input_vocabs=dvina_char_vocab,livvi_char_vocab
input_corpora=dvina_ids,livvi_ids
input_frequencies=dvina_freqs,livvi_freqs
distance=cos
min_token_prop=0.005
lang_names=dvi,liv

[sixgram_dvi_liv_anal]
type=langsim.modules.local_lm.embed_anal
input_model=sixgram_dvi_liv
input_vocabs=dvina_char_vocab,livvi_char_vocab
input_frequencies=dvina_freqs,livvi_freqs
min_token_prop=0.005
lang_names=dvi,liv
oov=OOV

[sixgram_dvi_liv_embeddings]
type=langsim.modules.local_lm.embeddings_from_model
input_model=sixgram_dvi_liv
input_vocabs=dvina_char_vocab,livvi_char_vocab
input_frequencies=dvina_freqs,livvi_freqs
lang_names=dvi,liv

[w2v_embeddings_dvi_liv]
type=pimlico.modules.embeddings.store_word2vec

[tsv_embeddings_dvi_liv_mixed]
type=langsim.modules.local_lm.store_tsv
input=sixgram_dvi_liv_embeddings




# Train on Ingrian and Livvi datasets from UHLCS
[sixgram_ing_liv_data]
type=langsim.modules.local_lm.neural_sixgram_samples
input_vocabs=ingrian_char_vocab,livvi_char_vocab
input_corpora=ingrian_ids,livvi_ids
input_frequencies=ingrian_freqs,livvi_freqs
oov=OOV

[sixgram_ing_liv]
type=langsim.modules.local_lm.neural_sixgram2
input_vocabs=ingrian_char_vocab,livvi_char_vocab
input_samples=sixgram_ing_liv_data
# Max epochs
epochs=30
modvar_lang_pair=join("_", lang)
# Increase patience of early stopping
patience=4
restarts=10


[plot_sixgram_ing_liv]
type=langsim.modules.local_lm.plot
input_model=sixgram_ing_liv
input_vocabs=ingrian_char_vocab,livvi_char_vocab
input_corpora=ingrian_ids,livvi_ids
input_frequencies=ingrian_freqs,livvi_freqs
distance=cos
min_token_prop=0.005
lang_names=ing,liv

[sixgram_ing_liv_anal]
type=langsim.modules.local_lm.embed_anal
input_model=sixgram_ing_liv
input_vocabs=ingrian_char_vocab,livvi_char_vocab
input_frequencies=ingrian_freqs,livvi_freqs
min_token_prop=0.005
lang_names=ing,liv
oov=OOV

[sixgram_ing_liv_embeddings]
type=langsim.modules.local_lm.embeddings_from_model
input_model=sixgram_ing_liv
input_vocabs=ingrian_char_vocab,livvi_char_vocab
input_frequencies=ingrian_freqs,livvi_freqs
lang_names=ing,liv

[w2v_embeddings_ing_liv]
type=pimlico.modules.embeddings.store_word2vec

[tsv_embeddings_ing_liv_mixed]
type=langsim.modules.local_lm.store_tsv
input=sixgram_ing_liv_embeddings




#### Train on Ingrian from UHLCS and Finnish Ylilauta ####

# Train on Ingrian and Livvi datasets from UHLCS
[sixgram_ing_fi_data]
type=langsim.modules.local_lm.neural_sixgram_samples
input_vocabs=ingrian_char_vocab,ylilauta_char_vocab
input_corpora=ingrian_ids,ylilauta_ids
input_frequencies=ingrian_freqs,ylilauta_freqs
oov=OOV

[sixgram_ing_fi]
type=langsim.modules.local_lm.neural_sixgram2
input_vocabs=ingrian_char_vocab,ylilauta_char_vocab
input_samples=sixgram_ing_fi_data
# Max epochs
epochs=30
modvar_lang_pair=join("_", lang)
# Increase patience of early stopping
patience=4
restarts=10


[plot_sixgram_ing_fi]
type=langsim.modules.local_lm.plot
input_model=sixgram_ing_fi
input_vocabs=ingrian_char_vocab,ylilauta_char_vocab
input_corpora=ingrian_ids,ylilauta_ids
input_frequencies=ingrian_freqs,ylilauta_freqs
distance=cos
min_token_prop=0.005
lang_names=ing,fi

[sixgram_ing_fi_anal]
type=langsim.modules.local_lm.embed_anal
input_model=sixgram_ing_fi
input_vocabs=ingrian_char_vocab,ylilauta_char_vocab
input_frequencies=ingrian_freqs,ylilauta_freqs
min_token_prop=0.005
lang_names=ing,fi
oov=OOV

[sixgram_ing_fi_embeddings]
type=langsim.modules.local_lm.embeddings_from_model
input_model=sixgram_ing_fi
input_vocabs=ingrian_char_vocab,ylilauta_char_vocab
input_frequencies=ingrian_freqs,ylilauta_freqs
lang_names=ing,fi

[w2v_embeddings_ing_fi]
type=pimlico.modules.embeddings.store_word2vec

[tsv_embeddings_ing_fi_mixed]
type=langsim.modules.local_lm.store_tsv
input=sixgram_ing_fi_embeddings

# Collect together analysis output
[collected_analysis]
type=pimlico.modules.utility.collect_files
input_files=sixgram_anal.analysis,
    sixgram_forum_anal_frequent.analysis,
    sixgram_forum_anal.analysis,
    sixgram_da_sv_anal.analysis,
    sixgram_dvi_liv_anal.analysis,
    sixgram_ing_liv_anal.analysis,
    sixgram_ing_fi_anal.analysis
names=fi_et,fi_et_forum,fi_et_forum_rare,da_sv,dvi_liv,ing_liv,ing_fi

# Collect together pair analysis output
[collected_pair_analysis]
type=pimlico.modules.utility.collect_files
input_files=sixgram_anal.pairs,
    sixgram_forum_anal_frequent.pairs,
    sixgram_forum_anal.pairs,
    sixgram_da_sv_anal.pairs,
    sixgram_dvi_liv_anal.pairs,
    sixgram_ing_liv_anal.pairs,
    sixgram_ing_fi_anal.pairs
names=fi_et,fi_et_forum,fi_et_forum_rare,da_sv,dvi_liv,ing_liv,ing_fi




### Europarl Spanish ###
[es_europarl]
type=langsim.modules.input.europarl
files=%(europarl_dir)s/es/*
modvar_lang="es"
encoding=utf-8
encoding_errors=ignore

[es_europarl_grouped]
type=pimlico.modules.corpora.tar_filter

[es_europarl_norm]
type=pimlico.modules.text.text_normalize
strip=T
case=lower
blank_lines=T

# Transform to char-level split text
[es_europarl_chars]
type=pimlico.modules.text.char_tokenize
input=es_europarl_norm
filter=T

# Build a vocab of the characters we need
[es_europarl_char_vocab]
type=pimlico.modules.corpora.vocab_builder
# Throw away chars we see fewer than 500 times in the corpus: they're probably just junk
threshold=500
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[es_europarl_ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=es_europarl_chars
input_vocab=es_europarl_char_vocab
oov=OOV

# Compute unigram distribution
[es_europarl_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=es_europarl_ids
input_vocab=es_europarl_char_vocab



### Europarl Portuguese ###
[pt_europarl]
type=langsim.modules.input.europarl
files=%(europarl_dir)s/pt/*
modvar_lang="pt"
encoding=utf-8
encoding_errors=ignore

[pt_europarl_grouped]
type=pimlico.modules.corpora.tar_filter

[pt_europarl_norm]
type=pimlico.modules.text.text_normalize
strip=T
case=lower
blank_lines=T

# Transform to char-level split text
[pt_europarl_chars]
type=pimlico.modules.text.char_tokenize
input=pt_europarl_norm
filter=T

# Build a vocab of the characters we need
[pt_europarl_char_vocab]
type=pimlico.modules.corpora.vocab_builder
# Throw away chars we see fewer than 500 times in the corpus: they're probably just junk
threshold=500
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[pt_europarl_ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=pt_europarl_chars
input_vocab=pt_europarl_char_vocab
oov=OOV

# Compute unigram distribution
[pt_europarl_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=pt_europarl_ids
input_vocab=pt_europarl_char_vocab




# Train on Spanish and Portuguese Europarl
[sixgram_es_pt_data]
type=langsim.modules.local_lm.neural_sixgram_samples
input_vocabs=es_europarl_char_vocab,pt_europarl_char_vocab
input_corpora=es_europarl_ids,pt_europarl_ids
input_frequencies=es_europarl_freqs,pt_europarl_freqs
oov=OOV

[sixgram_es_pt]
type=langsim.modules.local_lm.neural_sixgram2
input_vocabs=es_europarl_char_vocab,pt_europarl_char_vocab
input_samples=sixgram_es_pt_data
# Max epochs
epochs=30
modvar_lang_pair=join("_", lang)
# Increase patience of early stopping
patience=4
restarts=10


[sixgram_es_pt_embeddings]
type=langsim.modules.local_lm.embeddings_from_model
input_model=sixgram_es_pt
input_vocabs=es_europarl_char_vocab,pt_europarl_char_vocab
input_frequencies=es_europarl_freqs,pt_europarl_freqs
lang_names=es,pt

[sixgram_es_pt_embeddings_split]
type=langsim.modules.local_lm.lang_embeddings
input=sixgram_es_pt_embeddings
lang1=es

[w2v_embeddings_sixgram_es_pt_es]
type=pimlico.modules.embeddings.store_word2vec
input=sixgram_es_pt_embeddings_split.lang1_embeddings

[w2v_embeddings_sixgram_es_pt_pt]
type=pimlico.modules.embeddings.store_word2vec
input=sixgram_es_pt_embeddings_split.lang2_embeddings




### Europarl Spanish, using just a small subset ###

# Simulate low resourcedness, by taking just a small dataset
[es_europarl_small_chars]
type=pimlico.modules.corpora.subset
input=es_europarl_chars
size=100
skip_invalid=T

# Build a vocab of the characters we need
[es_europarl_small_char_vocab]
type=pimlico.modules.corpora.vocab_builder
# Throw away chars we see fewer than 30 times in the corpus: they're probably just junk
threshold=30
limit=100
# Include an OOV token at this stage, with the count of everything that was below the thresholds
oov=OOV

# Map to char IDs using the vocab
[es_europarl_small_ids]
type=pimlico.modules.corpora.vocab_mapper
input_text=es_europarl_small_chars
input_vocab=es_europarl_small_char_vocab
oov=OOV

# Compute unigram distribution
[es_europarl_small_freqs]
type=pimlico.modules.corpora.vocab_counter
input_corpus=es_europarl_small_ids
input_vocab=es_europarl_small_char_vocab



# Train on Spanish and Portuguese Europarl, using the very small Spanish set
[sixgram_es_small_pt_data]
type=langsim.modules.local_lm.neural_sixgram_samples
input_vocabs=es_europarl_small_char_vocab,pt_europarl_char_vocab
input_corpora=es_europarl_small_ids,pt_europarl_ids
input_frequencies=es_europarl_small_freqs,pt_europarl_freqs
oov=OOV

[sixgram_es_small_pt]
type=langsim.modules.local_lm.neural_sixgram2
input_vocabs=es_europarl_small_char_vocab,pt_europarl_char_vocab
input_samples=sixgram_es_small_pt_data
# Max epochs
epochs=30
modvar_lang_pair=join("_", lang)
# Increase patience of early stopping
patience=4
restarts=10

[plot_sixgram_es_small_pt]
type=langsim.modules.local_lm.plot
input_model=sixgram_es_small_pt
input_vocabs=es_europarl_small_char_vocab,pt_europarl_char_vocab
input_corpora=ylilauta_ids,est_ref_ids
input_frequencies=es_europarl_small_freqs,pt_europarl_freqs
distance=cos
min_token_prop=0.005
lang_names=es,pt

[sixgram_es_small_pt_anal]
type=langsim.modules.local_lm.embed_anal
input_model=sixgram_es_small_pt
input_vocabs=es_europarl_small_char_vocab,pt_europarl_char_vocab
input_frequencies=es_europarl_small_freqs,pt_europarl_freqs
min_token_prop=0.005
lang_names=es,pt
oov=OOV

[sixgram_es_small_pt_embeddings]
type=langsim.modules.local_lm.embeddings_from_model
input_model=sixgram_es_small_pt
input_vocabs=es_europarl_small_char_vocab,pt_europarl_char_vocab
input_frequencies=es_europarl_small_freqs,pt_europarl_freqs
lang_names=es,pt

[w2v_embeddings_sixgram_es_small_pt]
type=pimlico.modules.embeddings.store_word2vec

[sixgram_es_small_pt_embeddings_split]
type=langsim.modules.local_lm.lang_embeddings
input=sixgram_es_small_pt_embeddings
lang1=es

[w2v_embeddings_sixgram_es_small_pt_es]
type=pimlico.modules.embeddings.store_word2vec
input=sixgram_es_small_pt_embeddings_split.lang1_embeddings

[w2v_embeddings_sixgram_es_small_pt_pt]
type=pimlico.modules.embeddings.store_word2vec
input=sixgram_es_small_pt_embeddings_split.lang2_embeddings
