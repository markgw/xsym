<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Neural sixgram (Xsym) trainer, v1 &mdash; Xsym training 1.0 documentation</title>
    
    <link rel="stylesheet" href="../_static/pyramid.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/theme_overrides.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="Xsym training 1.0 documentation" href="../index.html" />
    <link rel="up" title="Symbol embedding methods" href="langsim.modules.local_lm.html" />
    <link rel="next" title="Neural sixgram (Xsym) trainer, v2" href="langsim.modules.local_lm.neural_sixgram2.html" />
    <link rel="prev" title="Language-specific embeddings" href="langsim.modules.local_lm.lang_embeddings.html" />
<!--[if lte IE 6]>
<link rel="stylesheet" href="../_static/ie6.css" type="text/css" media="screen" charset="utf-8" />
<![endif]-->

  </head>
  <body role="document">

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="langsim.modules.local_lm.neural_sixgram2.html" title="Neural sixgram (Xsym) trainer, v2"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="langsim.modules.local_lm.lang_embeddings.html" title="Language-specific embeddings"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Xsym training 1.0 documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="langsim.modules.html" >Pimlico modules</a> &raquo;</li>
          <li class="nav-item nav-item-2"><a href="langsim.modules.local_lm.html" accesskey="U">Symbol embedding methods</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="module-langsim.modules.local_lm.neural_sixgram">
<span id="neural-sixgram-xsym-trainer-v1"></span><h1>Neural sixgram (Xsym) trainer, v1<a class="headerlink" href="#module-langsim.modules.local_lm.neural_sixgram" title="Permalink to this headline">¶</a></h1>
<table border="1" class="docutils">
<colgroup>
<col width="23%" />
<col width="77%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Path</td>
<td>langsim.modules.local_lm.neural_sixgram</td>
</tr>
<tr class="row-even"><td>Executable</td>
<td>yes</td>
</tr>
</tbody>
</table>
<p>A special kind of six-gram model that combines 1-3 characters on the left with 1-3 characters on the right
to learn unigram, bigram and trigram representations.</p>
<p>This is one of the most successful representation learning methods among those here. It&#8217;s also very robust
across language pairs and different sizes of dataset. It&#8217;s therefore the model that I&#8217;ve opted to use in
subsequent work that uses the learned representations.</p>
<div class="section" id="inputs">
<h2>Inputs<a class="headerlink" href="#inputs" title="Permalink to this headline">¶</a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Name</th>
<th class="head">Type(s)</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>vocabs</td>
<td><code class="xref py py-class docutils literal"><span class="pre">list</span></code> of <code class="xref py py-class docutils literal"><span class="pre">Dictionary</span></code></td>
</tr>
<tr class="row-odd"><td>corpora</td>
<td><code class="xref py py-class docutils literal"><span class="pre">list</span></code> of TarredCorpus&lt;IntegerListsDocumentType&gt;</td>
</tr>
<tr class="row-even"><td>frequencies</td>
<td><code class="xref py py-class docutils literal"><span class="pre">list</span></code> of <code class="xref py py-class docutils literal"><span class="pre">NumpyArray</span></code></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="outputs">
<h2>Outputs<a class="headerlink" href="#outputs" title="Permalink to this headline">¶</a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Name</th>
<th class="head">Type(s)</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>model</td>
<td><code class="xref py py-class docutils literal"><span class="pre">KerasModelBuilderClass</span></code></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="options">
<h2>Options<a class="headerlink" href="#options" title="Permalink to this headline">¶</a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="2%" />
<col width="93%" />
<col width="5%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Name</th>
<th class="head">Description</th>
<th class="head">Type</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>embedding_size</td>
<td>Number of dimensions in the hidden representation. Default: 200</td>
<td>int</td>
</tr>
<tr class="row-odd"><td>plot_freq</td>
<td>Output plots to the output directory while training is in progress. This slows down training if it&#8217;s done very often. Specify how many batches to wait between each plot. Fewer means you get a finer grained picture of the training process, more means training goes faster. 0 (default) turns off plotting</td>
<td>int</td>
</tr>
<tr class="row-even"><td>context_weights</td>
<td>Coefficients that specify the relative frequencies with which each of the different lengths of contexts (1, 2 and 3) will be used in training examples. For each sample, a pair context lengths is selected at random. Six coefficients specify the weights given to (1,1), (1,2), (1,3), (2,2), (2,3) and (3,3). The opposite orderings have the same probability. By default, they are uniformly sampled (&#8216;1,1,1,1,1,1&#8217;), but you may adjust their relative frequencies to put more weight on some lengths than others. The first 6 values are the starting weights. After that, you may specify sets of 7 values: num_epochs, weight1, weight2, .... The weights at any point will transition smoothly (linearly) from the previous 6-tuple to the next, arriving at the epoch number given (i.e. 1=start of epoch 1 / end of first epoch). You may use float epoch numbers, e.g. 0.5</td>
<td>&lt;function context_weights at 0x7f3b52ef3050&gt;</td>
</tr>
<tr class="row-odd"><td>composition2_layers</td>
<td>Number and size of layers to use to combine pairs of characters, given as a list of integers. The final layer must be the same size as the embeddings, so is not included in this list</td>
<td>comma-separated list of ints</td>
</tr>
<tr class="row-even"><td>epochs</td>
<td>Max number of training epochs. Default: 5</td>
<td>int</td>
</tr>
<tr class="row-odd"><td>predictor_layers</td>
<td>Number and size of layers to use to take a pair of vectors and say whether they belong beside each other. Given as a list of integers. Doesn&#8217;t include the final projection to a single score</td>
<td>comma-separated list of ints</td>
</tr>
<tr class="row-even"><td>limit_training</td>
<td>Limit training to this many batches. Default: no limit</td>
<td>int</td>
</tr>
<tr class="row-odd"><td>l2_reg</td>
<td>L2 regularization to apply to all layers&#8217; weights. Default: 0.</td>
<td>float</td>
</tr>
<tr class="row-even"><td>unit_norm</td>
<td>If true, enforce a unit norm constraint on the learned embeddings. Default: false</td>
<td>bool</td>
</tr>
<tr class="row-odd"><td>word_internal</td>
<td>Only train model on word-internal sequences. Word boundaries will be included, but no sequences spanning over word boundaries</td>
<td>bool</td>
</tr>
<tr class="row-even"><td>dropout</td>
<td>Dropout to apply to embeddings during training. Default: 0.3</td>
<td>float</td>
</tr>
<tr class="row-odd"><td>oov</td>
<td>If given, use this special token in each vocabulary to represent OOVs. Otherwise, they are represented by an index added at the end of each vocabulary&#8217;s indices</td>
<td>string</td>
</tr>
<tr class="row-even"><td>word_boundary</td>
<td>If using word_internal, use this character (which must be in the vocabulary) to split words. Default: space</td>
<td>&lt;type &#8216;unicode&#8217;&gt;</td>
</tr>
<tr class="row-odd"><td>composition3_layers</td>
<td>Number and size of layers to use to combine triples of characters, given as a list of integers. The final layer must be the same size as the embeddings, so is not included in this list</td>
<td>comma-separated list of ints</td>
</tr>
<tr class="row-even"><td>store_all</td>
<td>Store updated representations from every epoch, even if the validation loss goes up. The default behaviour is to only store the parameters with best validation loss, but for these purposes we probably want to set this to T most of the time. (Defaults to F for backwards compatibility)</td>
<td>bool</td>
</tr>
<tr class="row-odd"><td>composition_dropout</td>
<td>Dropout to apply to composed representation during training. Default: same as dropout</td>
<td>float</td>
</tr>
<tr class="row-even"><td>batch</td>
<td>Training batch size. Default: 100</td>
<td>int</td>
</tr>
<tr class="row-odd"><td>sim_freq</td>
<td>How often (in batches) to compute the similarity of overlapping phonemes between the languages. -1 (default) means never, 0 means once at the start of each epoch</td>
<td>int</td>
</tr>
<tr class="row-even"><td>corpus_offset</td>
<td>To avoid training on parallel data, in the case where the input corpora happen to be parallel, jump forward in the second corpus by this number of utterances, putting the skipping utterances at the end instead. Default: 10k utterances</td>
<td>int</td>
</tr>
<tr class="row-odd"><td>cross_sentences</td>
<td>By default, the sliding window passed over the corpus stops at the end of a sentence (or whatever sequence division is in the input data) and starts again at the start of the next. Instead, join all sequences within a document into one long sequence and pass the sliding window over that</td>
<td>bool</td>
</tr>
<tr class="row-even"><td>validation</td>
<td>Number of samples to hold out as a validation set for training. Simply taken from the start of the corpus. Rounded to the nearest number of batches</td>
<td>int</td>
</tr>
<tr class="row-odd"><td>embedding_activation</td>
<td>Activation function to apply to the learned embeddings before they&#8217;re used, and also to every projection into the embedding space (the final layers of compositions). By default, &#8216;linear&#8217; is used, i.e. normal embeddings with no activation and a linear layer at the end of the composition functions. Choose any Keras named activation</td>
<td>string</td>
</tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Neural sixgram (Xsym) trainer, v1</a><ul>
<li><a class="reference internal" href="#inputs">Inputs</a></li>
<li><a class="reference internal" href="#outputs">Outputs</a></li>
<li><a class="reference internal" href="#options">Options</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="langsim.modules.local_lm.lang_embeddings.html"
                        title="previous chapter">Language-specific embeddings</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="langsim.modules.local_lm.neural_sixgram2.html"
                        title="next chapter">Neural sixgram (Xsym) trainer, v2</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="langsim.modules.local_lm.neural_sixgram2.html" title="Neural sixgram (Xsym) trainer, v2"
             >next</a> |</li>
        <li class="right" >
          <a href="langsim.modules.local_lm.lang_embeddings.html" title="Language-specific embeddings"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Xsym training 1.0 documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="langsim.modules.html" >Pimlico modules</a> &raquo;</li>
          <li class="nav-item nav-item-2"><a href="langsim.modules.local_lm.html" >Symbol embedding methods</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2018, Mark Granroth-Wilding.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.6.
    </div>
  </body>
</html>